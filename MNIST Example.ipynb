{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example\n",
    "Similar to how most programming languages have a \"Hello World\" exercise, machine learning has MNIST.\n",
    "\n",
    "MNIST is a simple computer vision dataset. It consists of imagess of handwritten digits like:\n",
    "\n",
    "(insert image)\n",
    "\n",
    "It also includes labels for each image, telling us what digit it is.\n",
    "\n",
    "We are going to train a model to look at the images and predict what digits they are. Let's start with a very simple model called Softmax Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "\n",
    "The MNIST data is hosted [here](http://yann.lecun.com/exdb/mnist/). To download and read in the data automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into three parts:\n",
    "1. 55,000 data points of training data (`mnist.train`)\n",
    "2. 10,000 data points of test data (`mnist.test`)\n",
    "3. 5,000 data points of validation data (`mnist.validation`)\n",
    "\n",
    "Every MNIST data has 2 parts:\n",
    "1. an image of a handwritten digit (call it \"x\")\n",
    "2. corresponding label (call it \"y\")\n",
    "\n",
    "Each image is 28px by 28px, we can interpert it as a big array of numbers. Let's flatten it into a vector of $28\\times 28=784$ numbers. \n",
    "\n",
    "From this perspective, the MNIST images are just a bunch of points in a 784-dimensional vector space. \n",
    "\n",
    "The result is that `mnist.train.images` is a tensor with the shape of `[55000, 784]`. The first dimension is an index to the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "\n",
    "Since we want the digit of a handwritten sample, we're going to want our labels as \"one-hot vectors\". A one-hot vector is a vector which is 0 in most dimensions and 1 in a single dimension. For example, 3 would be represented as `[0,0,0,1,0,0,0,0,0,0]`. So, `mnist.train.labels` is a `[55000, 10]` array of floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Regression\n",
    "\n",
    "Let's get cracking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick recap, `x` is a `placeholder`, a value that we will input when we ask TensorFlow to run a computation. We will represent the MNIST images as 2-D tensor of floating-point numbers, with a shape of `[None, 784]`, `None`, means that a dimension can be of any length.\n",
    "\n",
    "We also need the weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W` has a shape of `[784,10]` as we want to multiply the 784-dimensional image vectors by `W` to produce 10-dimensional vectors of evidence for the difference classes. `b` has a shape of `[10]` as we can add it to the output.\n",
    "\n",
    "We can now define our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We multiply `x` with `W` in that order as `x` has shape `[None, 784]` and `W` has shape `[784, 10]`. Small trick to deal with `x` being a 2D tensor with multiple inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will try to minimize the error, which represents how far off our model is from the our desired outcome. \n",
    "\n",
    "A common function to determine the loss of a model is called \"cross-entropy\". It is defined as:\n",
    "\n",
    "$$H_{y'}=-\\sum_{i} y_i' \\log(y_i)$$\n",
    "\n",
    "Where:\n",
    "- $y$ is our predicted probability distribution\n",
    "- $y'$ is the true distribution (one-hot vector with digit labels)\n",
    "\n",
    "To implement cross entropy we need to first add a new placeholder to input the correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the cross entropy function: $-\\sum_{i} y_i' \\log(y_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), \n",
    "                                              reduction_indices = [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.reduce_sum`\n",
    "\n",
    "computes the sum of elements across dimensions of a tensor.\n",
    "\n",
    "```python\n",
    "reduce_sum(\n",
    "    input_tensor,\n",
    "    axis=None,\n",
    "    keep_dims=False,\n",
    "    name=None,\n",
    "    reduction_indices=None\n",
    ")\n",
    "```\n",
    "For example:\n",
    "\n",
    "```python\n",
    "# 'x' is [[1, 1, 1]\n",
    "#         [1, 1, 1]]\n",
    "tf.reduce_sum(x) ==> 6\n",
    "tf.reduce_sum(x, 0) ==> [2, 2, 2]\n",
    "tf.reduce_sum(x, 1) ==> [3, 3]\n",
    "tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]\n",
    "tf.reduce_sum(x, [0, 1]) ==> 6\n",
    "```\n",
    "See [TensorFlow API](https://www.tensorflow.org/api_docs/python/tf/reduce_sum) for more details\n",
    "\n",
    "What is going on?\n",
    "\n",
    "1. compute logarithm of each element of `y`\n",
    "2. multiply each `y_` with the corresponding element of `tf.log(y)`\n",
    "3. `tf.reduce_sum` adds the elements in the second dimension of `y`\n",
    "4. `tf.reduce_mean` computes the mean over all the examples in the batch\n",
    "\n",
    "Let's implement the optimization algorithm. In this case, we are using gradient descent algorithm. There are [plenty](https://www.tensorflow.org/api_guides/python/train#Optimizers) more to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in range(400):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using small batches of random data is called stochastic training -- in this case, stochastic gradient descent. We would like to use train on the entire data set but that that's expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model\n",
    "\n",
    "Let's figure out if we predicted the right label. `tf.argmax` is an extrememly helpful function that returns the index of the highest entry in a tensor along some axis.\n",
    "\n",
    "`tf.argmax(y, 1)` is the predicted label while `tf.argmax(y_, 1)` is the actual label. We can use `tf.equal` to check if our prediction matches the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9113\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92% is not too bad but we can definitely do better. Stay tuned!\n",
    "\n",
    "### References\n",
    "- https://www.tensorflow.org/get_started/mnist/beginners\n",
    "- https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
